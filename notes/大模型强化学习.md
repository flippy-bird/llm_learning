## 大模型中的强化学习

### GSPO

> 参考1：[深度剖析Qwen3背后的RLHF新范式——GSPO, 解开万亿巨兽的“稳定枷锁”](https://zhuanlan.zhihu.com/p/1933456555954664522)
>
> 参考2：[GSPO：Qwen3 的 RL 秘方，奖励宜粗不宜细？](https://zhuanlan.zhihu.com/p/1932894729042917355)

qwen3中使用到的方法： 主要解决了GRPO在训练过程中的不稳定问题(尤其是在Moe模型的训练过程中)
- 与GRPO的区别，GRPO是在单个层面上进行调整，但是GSPO的思想是在整个句子或段落的层面上进行评估和优化。

GRPO的优化源于PPO，主要创新点在于使用组内平均的方式来评估每个sample的价值，从而消除了critic模型的使用，从而降低了训练的成本，但是GRPO在优化时，**还是延续了使用PPO的方式，是更新了每一个token**；GSPO觉得应该在整个句子或者段落层面上更新更为合理一些；





### 参考资料

1. [RL Infra架构演进](https://zhuanlan.zhihu.com/p/1951435056154386911)

